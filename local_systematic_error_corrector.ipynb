{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "def generate_data():\n",
    "    # Step 1: Generate synthetic data\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Parameters for synthetic dataset\n",
    "    n_samples_list = [100, 100, 100, 100, 100]\n",
    "    n_samples = sum(n_samples_list)\n",
    "    atom_types = [0, 1, 2]  # Example atom types\n",
    "    systematic_errors = [\n",
    "        {0: 10  , 1: -0.3, 2: 2 },\n",
    "        {0: 2   , 1:  3.1, 2: 0 },\n",
    "        {0: -1  , 1: -0.5, 2: -1},\n",
    "        {0: 0   , 1: -1.2, 2: 1 },\n",
    "        {0: -1.3, 1:  0.2, 2: 0 },\n",
    "    ]  # Referencing errors\n",
    "\n",
    "    # Generate features (e.g., structural properties) and true chemical shifts\n",
    "    X = np.random.rand(n_samples, 3) * 10  # Random features in the range [0, 10]\n",
    "    dataset_id = np.repeat(np.arange(len(n_samples_list)), np.array(n_samples_list))\n",
    "    true_cs = np.sin(3*X[:, 0] - 5*X[:, 1]) - 4*X[:, 1]**2 + X[:, 2] # + 0.1 * np.random.randn(n_samples)  # True shifts\n",
    "\n",
    "    # Assign atom types and introduce systematic errors\n",
    "    atom_type_labels_list = [np.random.choice(atom_types, size=n) for n in n_samples_list]\n",
    "    atom_type_labels = np.concatenate(atom_type_labels_list)\n",
    "\n",
    "    systematic_error = []\n",
    "    for batch, atom_types in enumerate(atom_type_labels_list):\n",
    "        systematic_error.append(np.array([systematic_errors[batch][atom] for atom in atom_types]))\n",
    "    systematic_error = np.concatenate(systematic_error)\n",
    "    adjusted_cs = true_cs + systematic_error\n",
    "\n",
    "    # Create a dataframe for clarity\n",
    "    data = pd.DataFrame({\n",
    "        'Feature1': X[:, 0],\n",
    "        'Feature2': X[:, 1],\n",
    "        'Feature3': X[:, 2],\n",
    "        'DatasetID': dataset_id,\n",
    "        'TrueCS': true_cs,\n",
    "        'AdjustedCS': adjusted_cs,\n",
    "        'AtomType': atom_type_labels\n",
    "    })\n",
    "\n",
    "    # Step 2: Train-test split\n",
    "    train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Separate features and targets\n",
    "    X_train, y_train, ID_train = train_data[['Feature1', 'Feature2', 'Feature3']], train_data['AdjustedCS'], train_data['DatasetID']\n",
    "    X_test , y_test , ID_test  = test_data [['Feature1', 'Feature2', 'Feature3']], test_data ['AdjustedCS'], test_data ['DatasetID']\n",
    "    atom_types_train = train_data['AtomType'].values\n",
    "    atom_types_test  = test_data ['AtomType'].values\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "    ID_train_tensor = torch.tensor(ID_train.values, dtype=torch.int8)\n",
    "    ID_test_tensor = torch.tensor(ID_test.values, dtype=torch.int8)\n",
    "    atom_types_train_tensor = torch.tensor(atom_types_train)\n",
    "    atom_types_test_tensor  = torch.tensor(atom_types_test)\n",
    "\n",
    "    return X_train_tensor, y_train_tensor, ID_train_tensor, atom_types_train_tensor, X_test_tensor, y_test_tensor, ID_test_tensor, atom_types_test_tensor\n",
    "\n",
    "def calculate_referencing_offset(y_true, y_pred, ID_tensor, atom_types):\n",
    "    \"\"\"\n",
    "    Calculate systematic referencing offsets for each atom type in the batch.\n",
    "    \"\"\"\n",
    "    offsets = {}\n",
    "    unique_atom_types = torch.unique(atom_types)\n",
    "    unique_dataset_ID = torch.unique(ID_tensor)\n",
    "    for atom in unique_atom_types:\n",
    "        for dataset_id in unique_dataset_ID:\n",
    "            mask = (atom_types == atom) * (ID_tensor == dataset_id)\n",
    "            if mask.sum() > 0:\n",
    "                offsets[atom.item(), dataset_id.item()] = torch.mean(y_true[mask] - y_pred[mask]).item()\n",
    "    return offsets\n",
    "\n",
    "def apply_offsets(y_pred, ID_tensor, atom_types, offsets, alpha=1.):\n",
    "    \"\"\"\n",
    "    Apply systematic referencing offsets to the target values.\n",
    "    \"\"\"\n",
    "    corrected_y = y_pred.clone()\n",
    "    for (atom, dataset_id), offset in offsets.items():\n",
    "        mask = (atom_types == atom) * (ID_tensor == dataset_id)\n",
    "        corrected_y[mask] += alpha * offset\n",
    "    return corrected_y\n",
    "\n",
    "def train(model, criterion, optimizer, X_train_tensor, y_train_tensor, ID_train_tensor,\n",
    "          atom_types_train_tensor, epochs, use_rereferencing, alpha_breakeven_epoch=0):\n",
    "    model.train()\n",
    "    for epoch in range(1, epochs+1):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        # Forward pass\n",
    "        y_pred_train = model(X_train_tensor, atom_types_train_tensor)\n",
    "    \n",
    "        if use_rereferencing:\n",
    "            # Calculate referencing offsets\n",
    "            offsets = calculate_referencing_offset(\n",
    "                y_train_tensor, \n",
    "                y_pred_train,\n",
    "                ID_train_tensor,\n",
    "                atom_types_train_tensor,\n",
    "            )\n",
    "            alpha = np.tanh(epoch - alpha_breakeven_epoch)/2 + 0.5\n",
    "            # Apply offsets to targets\n",
    "            y_pred_train = apply_offsets(y_pred_train, ID_train_tensor, atom_types_train_tensor, offsets, alpha).view(-1, 1)\n",
    "    \n",
    "        # Recompute loss after offset adjustment\n",
    "        loss = criterion(y_pred_train, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Print progress\n",
    "        if epoch % 1000 == 0:\n",
    "            print(f\"Epoch {epoch}/{epochs}, Loss: {loss.item()}\")\n",
    "\n",
    "def evaluate(model, X_test_tensor, y_test_tensor, ID_test_tensor, atom_types_test_tensor):\n",
    "\n",
    "    # Step 6: Evaluate on Test Set\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_pred_test  = model(X_test_tensor, atom_types_test_tensor)\n",
    "\n",
    "    # Correct test targets using calculated offsets\n",
    "    test_offsets  = calculate_referencing_offset(\n",
    "        y_test_tensor,\n",
    "        y_pred_test,\n",
    "        ID_test_tensor,\n",
    "        atom_types_test_tensor)\n",
    "    y_pred_test_corrected = apply_offsets(y_pred_test, ID_test_tensor, atom_types_test_tensor, test_offsets)\n",
    "\n",
    "    # Compute errors\n",
    "    original_test_error  = mean_squared_error(y_test_tensor, y_pred_test)\n",
    "    corrected_test_error = mean_squared_error(y_test_tensor, y_pred_test_corrected)\n",
    "\n",
    "    # Step 7: Visualization\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test_tensor, y_pred_test, alpha=0.7, label='Original Targets')\n",
    "    plt.scatter(y_test_tensor, y_pred_test_corrected, alpha=0.7, label='Corrected Targets')\n",
    "    plt.plot([min(y_test_tensor), max(y_test_tensor)], [min(y_test_tensor), max(y_test_tensor)], color='red', linestyle='--', label='Ideal')\n",
    "    plt.xlabel('True Chemical Shifts')\n",
    "    plt.ylabel('Predicted Chemical Shifts')\n",
    "    plt.title('MLP Model Performance Before and After Target Correction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(original_test_error, corrected_test_error)\n",
    "\n",
    "# Step 2: Define MLP Model\n",
    "class ResidualMLPModel(nn.Module):\n",
    "\n",
    "    # Step 1: Custom Activation Functions\n",
    "    class SinActivation(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return torch.sin(x)\n",
    "\n",
    "    class Pow2Activation(nn.Module):\n",
    "        def forward(self, x):\n",
    "            return x ** 2\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_types, embedding_dim=8):\n",
    "        super(ResidualMLPModel, self).__init__()\n",
    "        self.embedder = torch.nn.Embedding(num_types, embedding_dim=embedding_dim)\n",
    "        self.input_layer = nn.Linear(input_dim + embedding_dim, hidden_dim, bias=False)\n",
    "        self.sin_activation = ResidualMLPModel.SinActivation()\n",
    "        self.hidden_layer_1 = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.pow2_activation = ResidualMLPModel.Pow2Activation()\n",
    "        self.hidden_layer_2 = nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.output_layer = nn.Linear(hidden_dim, output_dim, bias=False)\n",
    "    \n",
    "    def forward(self, x, atom_types):\n",
    "\n",
    "        embedding = self.embedder(atom_types)\n",
    "\n",
    "        # First layer with activation\n",
    "        o1 = self.input_layer(torch.cat([x, embedding], dim=-1))\n",
    "        o2 = self.sin_activation(o1) # torch.nn.functional.relu(x)\n",
    "        \n",
    "        # Second layer with residual connection and activation\n",
    "        o3 = self.hidden_layer_1(x)\n",
    "        o4 = self.pow2_activation(o3) + o2 # torch.nn.functional.relu(x)\n",
    "\n",
    "        # Third layer with residual connection and activation\n",
    "        o5 = self.hidden_layer_2(x) + o4\n",
    "        \n",
    "        # Final output layer\n",
    "        return self.output_layer(o5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor, y_train_tensor, ID_train_tensor, atom_types_train_tensor, X_test_tensor, y_test_tensor, ID_test_tensor, atom_types_test_tensor = generate_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize Model, Loss, and Optimizer\n",
    "mlp_model = ResidualMLPModel(input_dim=X_train_tensor.shape[1], hidden_dim=32, output_dim=1, num_types=3, embedding_dim=8)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(mlp_model, criterion, optimizer, X_train_tensor, y_train_tensor, ID_train_tensor,\n",
    "      atom_types_train_tensor, epochs=20000, use_rereferencing=True, alpha_breakeven_epoch=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(mlp_model, X_test_tensor, y_test_tensor, ID_test_tensor, atom_types_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Initialize Model, Loss, and Optimizer\n",
    "base_mlp_model = ResidualMLPModel(input_dim=X_train_tensor.shape[1], hidden_dim=128, output_dim=1, num_types=3, embedding_dim=64)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(base_mlp_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train(base_mlp_model, criterion, optimizer, X_train_tensor, y_train_tensor, ID_train_tensor, atom_types_train_tensor, epochs=10000, use_rereferencing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(base_mlp_model, X_test_tensor, y_test_tensor, ID_test_tensor, atom_types_test_tensor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
