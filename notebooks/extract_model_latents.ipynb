{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from geqtrain.scripts.evaluate import load_model, infer\n",
    "from geqtrain.utils import Config\n",
    "from geqtrain.data import AtomicDataDict\n",
    "from geqtrain.data._build import dataset_from_config\n",
    "from geqtrain.data.dataloader import DataLoader as GQTDataloader\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = '/home/angiod@usi.ch/CSnet/results/DFT/H.mlp.noise.r7/best_model.pth'\n",
    "TEST_CONFIG = None\n",
    "DEVICE = 'cuda:3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hidden_dataset(model, test_config=None, device='cpu'):\n",
    "    # Load model\n",
    "    model, config = load_model(model, device=device)\n",
    "    n_scalars = config.get(\"latent_dim\")\n",
    "\n",
    "    if test_config is not None:\n",
    "        config.update(Config.from_file(test_config, defaults={}))\n",
    "    \n",
    "    # Load datasets\n",
    "    try:\n",
    "        train_dataset      = dataset_from_config(config, prefix=\"dataset\")\n",
    "    except KeyError:\n",
    "        train_dataset      = None\n",
    "    try:\n",
    "        validation_dataset = dataset_from_config(config, prefix=\"validation_dataset\")\n",
    "    except KeyError:\n",
    "        validation_dataset = None\n",
    "    try:\n",
    "        test_dataset       = dataset_from_config(config, prefix=\"test_dataset\")\n",
    "    except KeyError:\n",
    "        test_dataset       = None\n",
    "    \n",
    "    # Iterate datasets\n",
    "    for category, dataset in zip(['train', 'valid', 'test'], [train_dataset, validation_dataset, test_dataset]):\n",
    "        if dataset is None:\n",
    "            continue\n",
    "\n",
    "        # Build dataloader\n",
    "        dataloader = GQTDataloader(\n",
    "            dataset=dataset,\n",
    "            shuffle=False,\n",
    "            batch_size=1,\n",
    "        )\n",
    "\n",
    "        root = f'./hidden_dataset/{config[\"run_name\"]}'\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "\n",
    "        x, y, n = [], [], []\n",
    "\n",
    "        def out_callback(batch_index, chunk_index, out, ref_data, pbar, **kwargs): # Keep **kwargs or callback fails\n",
    "            x.append(out[AtomicDataDict.NODE_FEATURES_KEY][:, :n_scalars].cpu().numpy())\n",
    "            y.append(ref_data["node_output"].cpu().numpy())\n",
    "            n.append(ref_data[AtomicDataDict.NODE_TYPE_KEY].cpu().numpy())\n",
    "\n",
    "        # Run inference and save hidden features and targets\n",
    "        infer(dataloader, model, DEVICE, chunk_callbacks=[out_callback], **config)\n",
    "\n",
    "        x = np.concatenate(x, axis=0)\n",
    "        y = np.concatenate(y, axis=0)\n",
    "        n = np.concatenate(n, axis=0)\n",
    "        np.savez(os.path.join(root, f'dataset.{category}.npz'), x=x, y=y, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hidden_dataset(MODEL, TEST_CONFIG, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KEEP_NODE_TYPE = 1\n",
    "DEVICE = 'cuda:3'\n",
    "BATCH_SIZE = 512\n",
    "GRID_SIZE = 500  # Grid points for SKI\n",
    "\n",
    "# gpytorch.settings.max_cg_iterations(50)  # Set a reasonable CG iteration limit\n",
    "# gpytorch.settings.fast_computations(covar_root_decomposition=True, log_prob=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def std_scaler(X):\n",
    "    X_mean, X_std = X.mean(), X.std()\n",
    "    return (X - X_mean) / X_std, X_mean, X_std\n",
    "\n",
    "# --- 1. Load Training and Validation Data ---\n",
    "def load_data(path, keep_node_type: int = None, device='cpu'):\n",
    "    df = np.load(path, allow_pickle=True)\n",
    "    node_types = df['n'].flatten()\n",
    "    if keep_node_type is not None:\n",
    "        fltr = node_types == keep_node_type\n",
    "    else:\n",
    "        fltr = np.arange(len(node_types))\n",
    "    x = torch.from_numpy(df['x']) # shape (N, hidden)\n",
    "    y = torch.from_numpy(df['y']) # shape (N, 1)\n",
    "    fltr = torch.from_numpy(fltr) * ~torch.any(torch.isnan(y), dim=1)\n",
    "    \n",
    "    x, x_mean, x_std = std_scaler(x[fltr].to(device))\n",
    "    y, y_mean, y_std = std_scaler(y[fltr].flatten().to(device))\n",
    "    return x, x_mean, x_std, y, y_mean, y_std\n",
    "\n",
    "# Load your data (provide correct paths to CSV files)\n",
    "train_x, train_x_mean, train_x_std, train_y, train_y_mean, train_y_std = load_data('hidden_dataset/H.mlp.noise.r7/dataset.train.npz', keep_node_type=KEEP_NODE_TYPE, device=DEVICE)\n",
    "train_x = train_x[:1024]\n",
    "train_y = train_y[:1024]\n",
    "# train_x = torch.rand_like(train_x)\n",
    "num_data, num_dims = train_x.size()  # Dimensionality of input space\n",
    "\n",
    "# --- 2. Define the GP Model ---\n",
    "\n",
    "# --- A) Exact GP ---\n",
    "class GPRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# --- B) Exact GP with Grid-Interpolation Kernel (SKI) ---\n",
    "class SKIRegressionModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(SKIRegressionModel, self).__init__(train_x, train_y, likelihood)\n",
    "        dims = train_x.size(1)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(gpytorch.kernels.RBFKernel(ard_num_dims=dims), grid_size=100, num_dims=dims)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# --- C) Approximated GP with Grid-Interpolation Kernel (SKI) and Stochastic Variational Gaussian Processes (SVGP) => Stochastic Variational SKI (SV-SKI) ---\n",
    "class SVSKIRegressionModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, grid_size, num_dims):\n",
    "        \n",
    "        # Define inducing points on a grid\n",
    "        inducing_grid = torch.zeros(grid_size, num_dims)\n",
    "\n",
    "        # Variational distribution over inducing points\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_grid.size(0))\n",
    "\n",
    "        # Variational strategy\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(\n",
    "            self, inducing_grid, variational_distribution, learn_inducing_locations=False\n",
    "        )\n",
    "        super(SVSKIRegressionModel, self).__init__(variational_strategy)\n",
    "\n",
    "        # Mean and covariance modules\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.AdditiveStructureKernel(\n",
    "            gpytorch.kernels.GridInterpolationKernel(gpytorch.kernels.RBFKernel(), grid_size=grid_size, num_dims=1),\n",
    "            num_dims=num_dims,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# --- D) Approximated GP with Stochastic Variational Gaussian Processes (SVGP) ---\n",
    "class VGPRegressionModel(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = gpytorch.variational.CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = gpytorch.variational.VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(VGPRegressionModel, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize GP components\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "\n",
    "gp_model = GPRegressionModel(train_x, train_y, likelihood)\n",
    "# gp_model = SKIRegressionModel(train_x, train_y, likelihood)\n",
    "# gp_model = SVSKIRegressionModel(GRID_SIZE, num_dims)\n",
    "# gp_model = VGPRegressionModel(inducing_points=train_x[:500])\n",
    "\n",
    "# --- 3. Train the GP ---\n",
    "gp_model.to(DEVICE)\n",
    "gp_model.train()\n",
    "likelihood.to(DEVICE)\n",
    "likelihood.train()\n",
    "\n",
    "# Define optimizer and loss\n",
    "optimizer = torch.optim.Adam([\n",
    "    # {'params': gp_model.feature_extractor.parameters()},\n",
    "    {'params': gp_model.covar_module.parameters()},\n",
    "    {'params': gp_model.mean_module.parameters()},\n",
    "    # {'params': gp_model.likelihood.parameters()},\n",
    "], lr=0.01)\n",
    "\n",
    "# mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, gp_model)\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, gp_model, num_data=num_data)\n",
    "\n",
    "# Training loop\n",
    "training_epochs = 10000\n",
    "for epoch in range(training_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = gp_model(train_x)\n",
    "    loss = -mll(output, train_y).mean()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{training_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# train_dataset = TensorDataset(train_x, train_y)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# # Training loop\n",
    "# training_epochs = 20000\n",
    "# for epoch in range(training_epochs):\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         x_batch, y_batch = x_batch.to(DEVICE), y_batch.to(DEVICE)\n",
    "        \n",
    "#         optimizer.zero_grad()\n",
    "#         output = gp_model(x_batch)\n",
    "#         loss = -mll(output, y_batch).mean()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "    \n",
    "#     if (epoch + 1) % 1 == 0:\n",
    "#         print(f\"Epoch {epoch + 1}/{training_epochs}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 4. Evaluate on Validation Set ---\n",
    "\n",
    "gp_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    # Load your data (provide correct paths to CSV files)\n",
    "    val_x, val_x_mean, val_x_std, val_y, val_y_mean, val_y_std = load_data('hidden_dataset/pretraining.small/dataset.valid.npz', keep_node_type=KEEP_NODE_TYPE, device=DEVICE)\n",
    "    preds = likelihood(gp_model(val_x))\n",
    "    mean = preds.mean * train_y_std + train_y_mean\n",
    "    lower, upper = preds.confidence_region()\n",
    "\n",
    "# --- 5. Visualize Predictions ---\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "target = val_y * val_y_std + val_y_mean\n",
    "target = target.cpu().numpy()\n",
    "sorted_idcs = np.argsort(target)\n",
    "\n",
    "# True vs Predicted\n",
    "plt.plot(target[sorted_idcs][:20], label=\"True Targets\", color=\"blue\", marker='o', linestyle='None')\n",
    "plt.plot(mean.cpu().numpy()[sorted_idcs][:20], label=\"Predicted Mean\", color=\"orange\", marker='x', linestyle='None')\n",
    "\n",
    "# Confidence intervals\n",
    "plt.fill_between(\n",
    "    range(len(mean))[:20],\n",
    "    mean.cpu().numpy()[:20] + lower.cpu().numpy()[:20],\n",
    "    mean.cpu().numpy()[:20] + upper.cpu().numpy()[:20],\n",
    "    color=\"orange\",\n",
    "    alpha=0.3,\n",
    "    label=\"Confidence Interval\"\n",
    ")\n",
    "\n",
    "plt.title(\"Gaussian Process Regression: Validation Set\")\n",
    "plt.xlabel(\"Sample Index\")\n",
    "plt.ylabel(\"Target Value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# # Step 1: Generate synthetic dataset\n",
    "# # Dataset with 8 features, 1 target value, 100 samples\n",
    "# X, y = make_regression(n_samples=100, n_features=8, noise=0.1, random_state=42)\n",
    "\n",
    "# # Step 2: Split the dataset into training and test sets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train, train_x_mean, train_x_std, y_train, train_y_mean, train_y_std = load_data('hidden_dataset/H.mlp.noise.r7/dataset.train.npz', keep_node_type=KEEP_NODE_TYPE, device='cpu')\n",
    "X_train = X_train[:1024]\n",
    "y_train = y_train[:1024]\n",
    "train_y_mean = train_y_mean.numpy()\n",
    "train_y_std = train_y_std.numpy()\n",
    "\n",
    "X_test, _, _, y_test, m, s = load_data('hidden_dataset/H.mlp.noise.r7/dataset.valid.npz', keep_node_type=KEEP_NODE_TYPE, device='cpu')\n",
    "m = m.numpy()\n",
    "s = s.numpy()\n",
    "\n",
    "# Step 3: Define the Kernel Ridge Regression model\n",
    "# Using a Gaussian (RBF) kernel and regularization parameter alpha\n",
    "b = 100\n",
    "for a in range(0,100,10):\n",
    "    aa = a * 0.01\n",
    "    for g in range(0,100,10):\n",
    "        gg = g * 0.01\n",
    "        model = KernelRidge(alpha=aa, kernel='rbf', gamma=gg)\n",
    "\n",
    "        # Step 4: Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Step 5: Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Step 6: Evaluate the model\n",
    "        mse = mean_squared_error(y_test * s + m, y_pred * s + m)\n",
    "        if mse < b:\n",
    "            b = mse\n",
    "            print(aa, gg, mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test * s + m)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred * s + m)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Make predictions\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "# Step 6: Evaluate the model\n",
    "mse = mean_squared_error(y_train * train_y_std + train_y_mean, y_pred * train_y_std + train_y_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_test * train_y_std + train_y_mean)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(y_pred * train_y_std + train_y_mean)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "csnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
